{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.utils.ffmpeg_utils.get_audio_decoders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_mel_spectrogram(root_dir, output_dir, **mel_params):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(**mel_params)\n",
    "    amp_to_db = torchaudio.transforms.AmplitudeToDB(top_db=80.0)\n",
    "\n",
    "    audio_files = [f for f in os.listdir(root_dir) if f.endswith((\".mp3\", \".wav\", \".flac\"))]\n",
    "\n",
    "    for audio_file in tqdm(audio_files):\n",
    "        waveform, sr = torchaudio.load(os.path.join(root_dir, audio_file))\n",
    "\n",
    "        if sr != mel_params[\"sample_rate\"]:\n",
    "            resampler = torchaudio.transforms.Resample(sr, mel_params[\"sample_rate\"])\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Compute mel spectrogram\n",
    "        mel_spec = mel_transform(waveform)\n",
    "        # Convert to decibels\n",
    "        mel_spec = amp_to_db(mel_spec)\n",
    "        # Normalize\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / mel_spec.std()\n",
    "\n",
    "        # Save to disk\n",
    "        torch.save(mel_spec, os.path.join(output_dir, f\"{os.path.splitext(audio_file)[0]}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9b55a456b244428b6cf418a13b51fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mel_params = {\n",
    "    \"sample_rate\": 32000,\n",
    "    \"n_fft\": 2048,\n",
    "    \"hop_length\": 512,\n",
    "    \"n_mels\": 160,\n",
    "    \"f_min\": 20,\n",
    "    \"f_max\": 16000,\n",
    "    \"power\": 2.0,\n",
    "}\n",
    "\n",
    "precompute_mel_spectrogram(\"./data/download\", \"./data/mel_spectrograms\", **mel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load(\"./data/mel_spectrograms/yes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 160, 4028])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training: Keep complete chunks for stable VQ-VAE training\n",
    "# During inference/feature extraction: Handle partial chunks by either:\n",
    "# a) Zero-padding (simpler) \n",
    "# b) Overlapping strategy for the last chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, chunk_size, overlap):\n",
    "        self.root_dir = root_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "        self.audio_files = [f for f in os.listdir(root_dir) if f.endswith(\".pt\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mel_spec = torch.load(os.path.join(self.root_dir, self.audio_files[idx]))\n",
    "        mel_spec = mel_spec.squeeze(0)\n",
    "\n",
    "        chunks = []\n",
    "        step_size = self.chunk_size - self.overlap\n",
    "        for start in range(0, mel_spec.size(1) - self.chunk_size + 1, step_size):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = mel_spec[:, start:end]\n",
    "            chunks.append(chunk)\n",
    "        return torch.stack(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(\"./data/mel_spectrograms\", chunk_size=512, overlap=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 160, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dims, n_embed):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, hidden_dims, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dims, hidden_dims, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dims, n_embed, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, n_embed, embed_dim, beta=0.25):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.embed_dim = embed_dim\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_embed, embed_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / n_embed, 1.0 / n_embed)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Reshape z -> (batch, height, width, channel)\n",
    "        z = z.permute(0, 2, 1).contiguous()\n",
    "        z_flattened = z.view(-1, self.embed_dim)\n",
    "\n",
    "        # Distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2ze\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
    "            2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        # Find nearest encoding\n",
    "        min_encoding_indices = torch.argmin(d, dim=1)\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
    "\n",
    "        # Preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # Compute losses\n",
    "        commitment_loss = F.mse_loss(z_q.detach(), z)\n",
    "        codebook_loss = F.mse_loss(z_q, z.detach())\n",
    "        loss = codebook_loss + self.beta * commitment_loss\n",
    "\n",
    "        # Reshape back to match original input shape\n",
    "        z_q = z_q.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        return z_q, loss, min_encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dims, out_channels):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels, hidden_dims, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(hidden_dims, hidden_dims, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(hidden_dims, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dims=128, n_embed=512, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_dims, embed_dim)\n",
    "        self.vector_quantizer = VectorQuantizer(n_embed, embed_dim)\n",
    "        self.decoder = Decoder(embed_dim, hidden_dims, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, vq_loss, indices = self.vector_quantizer(z)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        \n",
    "        return x_recon, vq_loss, indices\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, _, indices = self.vector_quantizer(z)\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices):\n",
    "        z_q = self.vector_quantizer.embedding(indices)\n",
    "        z_q = z_q.permute(0, 2, 1)  # Reshape for decoder\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE(in_channels=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv1d(160, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): ReLU()\n",
       "      (4): Conv1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (vector_quantizer): VectorQuantizer(\n",
       "    (embedding): Embedding(512, 64)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): ConvTranspose1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose1d(128, 160, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x):\n",
    "    optimizer.zero_grad()\n",
    "    x_recon, vq_loss, _ = model(x)\n",
    "    recon_loss = F.mse_loss(x_recon, x)\n",
    "    loss = recon_loss + vq_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, recon_loss, vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e0b270aa23441d98f77eaf0b60d55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 141755121664.0 Recon Loss: 0.48870864510536194 VQ Loss: 141755121664.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71b24761aa1460c9fce6d9a00cf20ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 309209497600.0 Recon Loss: 0.48867377638816833 VQ Loss: 309209497600.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e6b983d2d947abbd486f847be2a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 626874253312.0 Recon Loss: 0.4885919988155365 VQ Loss: 626874253312.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10267e9e475e45f387cc9b6327540f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 1194047832064.0 Recon Loss: 0.4885203242301941 VQ Loss: 1194047832064.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f29121c66e4da494537983ff89e59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 2155812487168.0 Recon Loss: 0.48846906423568726 VQ Loss: 2155812487168.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4c2bc5b8f3403981fa582c11a6766d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 3716136566784.0 Recon Loss: 0.48841795325279236 VQ Loss: 3716136566784.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124294e07e6946329460e34618ebb463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 6152592556032.0 Recon Loss: 0.4883686602115631 VQ Loss: 6152592556032.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d480428eb67f44ab903acc139e84ec8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 9832612495360.0 Recon Loss: 0.48832207918167114 VQ Loss: 9832612495360.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747255d966e0473f8878281a9065f358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 15231238537216.0 Recon Loss: 0.48827728629112244 VQ Loss: 15231238537216.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf50e54cf9a4f5486abf87633d54908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 22949773967360.0 Recon Loss: 0.48823362588882446 VQ Loss: 22949773967360.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss, recon_loss, vq_loss = [], [], []\n",
    "\n",
    "    model.train()\n",
    "    for x in tqdm(dataset):\n",
    "        for chunk in x:\n",
    "            chunk = chunk.to(\"cuda\")\n",
    "            chunk = chunk.unsqueeze(0)\n",
    "\n",
    "            l, r, v = train_step(chunk)\n",
    "            loss.append(l.item())\n",
    "            recon_loss.append(r.item())\n",
    "            vq_loss.append(v.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {torch.tensor(loss).mean().item()} Recon Loss: {torch.tensor(recon_loss).mean().item()} VQ Loss: {torch.tensor(vq_loss).mean().item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
